{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ibura\\Manifold SPC Paper v3\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the parent directory (ManifoldSPC)\n",
    "project_dir = os.path.abspath(os.path.join(os.getcwd(), os.pardir))\n",
    "\n",
    "# Add the project directory to sys.path\n",
    "sys.path.append(project_dir)\n",
    "print(project_dir)\n",
    "\n",
    "from scripts2.DR_methods import *\n",
    "from scripts2.Filters import *\n",
    "from scripts2.functions import *\n",
    "from scripts2.TE_functions_v6 import parse_filename\n",
    "from ControlChart.control_chart import DFEWMA\n",
    "from ControlChart.control_chart import DME\n",
    "# from scripts.YSL23_test2 import YSL23 as MF\n",
    "from scripts2.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnsadd_stddev = np.array([\n",
    "    0.01,   # xmeasadd[0]\n",
    "    0.01,   # xmeasadd[1]\n",
    "    0.01,   # xmeasadd[2]\n",
    "    0.01,   # xmeasadd[3]\n",
    "    0.01,   # xmeasadd[4]\n",
    "    0.125,  # xmeasadd[5]\n",
    "    0.01,   # xmeasadd[6]\n",
    "    0.125,  # xmeasadd[7]\n",
    "    0.01,   # xmeasadd[8]\n",
    "    0.01,   # xmeasadd[9]\n",
    "    0.25,   # xmeasadd[10]\n",
    "    0.1,    # xmeasadd[11]\n",
    "    0.25,   # xmeasadd[12]\n",
    "    0.1,    # xmeasadd[13]\n",
    "    0.25,   # xmeasadd[14]\n",
    "    0.025,  # xmeasadd[15]\n",
    "    0.25,   # xmeasadd[16]\n",
    "    0.1,    # xmeasadd[17]\n",
    "    0.25,   # xmeasadd[18]\n",
    "    0.1,    # xmeasadd[19]\n",
    "    0.25,   # xmeasadd[20]\n",
    "    0.025,  # xmeasadd[21]\n",
    "    0.25,   # xmeasadd[22]\n",
    "    0.1,    # xmeasadd[23]\n",
    "    0.25,   # xmeasadd[24]\n",
    "    0.1,    # xmeasadd[25]\n",
    "    0.25,   # xmeasadd[26]\n",
    "    0.025,  # xmeasadd[27]\n",
    "    0.25,   # xmeasadd[28]\n",
    "    0.1,    # xmeasadd[29]\n",
    "    0.25,   # xmeasadd[30]\n",
    "    0.1     # xmeasadd[31]\n",
    "])\n",
    "\n",
    "xns_stddev = np.array([\n",
    "    0.0012,  # xmeas[0]\n",
    "    18.0,    # xmeas[1]\n",
    "    22.0,    # xmeas[2]\n",
    "    0.05,    # xmeas[3]\n",
    "    0.2,     # xmeas[4]\n",
    "    0.21,    # xmeas[5]\n",
    "    0.3,     # xmeas[6]\n",
    "    0.5,     # xmeas[7]\n",
    "    0.01,    # xmeas[8]\n",
    "    0.0017,  # xmeas[9]\n",
    "    0.01,    # xmeas[10]\n",
    "    1.0,     # xmeas[11]\n",
    "    0.3,     # xmeas[12]\n",
    "    0.125,   # xmeas[13]\n",
    "    1.0,     # xmeas[14]\n",
    "    0.3,     # xmeas[15]\n",
    "    0.115,   # xmeas[16]\n",
    "    0.01,    # xmeas[17]\n",
    "    1.15,    # xmeas[18]\n",
    "    0.2,     # xmeas[19]\n",
    "    0.01,    # xmeas[20]\n",
    "    0.01,    # xmeas[21]\n",
    "    0.25,    # xmeas[22]\n",
    "    0.1,     # xmeas[23]\n",
    "    0.25,    # xmeas[24]\n",
    "    0.1,     # xmeas[25]\n",
    "    0.25,    # xmeas[26]\n",
    "    0.025,   # xmeas[27]\n",
    "    0.25,    # xmeas[28]\n",
    "    0.1,     # xmeas[29]\n",
    "    0.25,    # xmeas[30]\n",
    "    0.1,     # xmeas[31]\n",
    "    0.25,    # xmeas[32]\n",
    "    0.025,   # xmeas[33]\n",
    "    0.05,    # xmeas[34]\n",
    "    0.05,    # xmeas[35]\n",
    "    0.01,    # xmeas[36]\n",
    "    0.01,    # xmeas[37]\n",
    "    0.01,    # xmeas[38]\n",
    "    0.5,     # xmeas[39]\n",
    "    0.5      # xmeas[40]\n",
    "])\n",
    "\n",
    "keep_mask = np.array([\n",
    "    0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
    "    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
    "    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
    "    1, 1, 0, 1, 1, 0\n",
    "])\n",
    "\n",
    "obs_order = np.hstack([\n",
    "    np.array([1]),\n",
    "    xnsadd_stddev,\n",
    "    xns_stddev,\n",
    "    np.ones(12)\n",
    "])\n",
    "\n",
    "keep_mask_temp = np.hstack([\n",
    "    np.ones(30),\n",
    "    np.zeros(9)\n",
    "]).astype(bool) \n",
    "\n",
    "obs_order = obs_order[keep_mask==1]\n",
    "# print(obs_order.shape)\n",
    "obs_order = np.tile(obs_order, (10, 1)).reshape(-1)\n",
    "keep_mask_temp = np.tile(keep_mask_temp,(10, 1)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = np.loadtxt(\"dataIC.csv\", delimiter=\",\")\n",
    "d = d[:20000]/obs_order\n",
    "d = d[:,keep_mask_temp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12294.3349761742\n",
      "8.133827506229084e-05\n"
     ]
    }
   ],
   "source": [
    "n_warm_up = 5000\n",
    "n_manifold = 700 + n_warm_up\n",
    "n_var = 400\n",
    "n_ic = n_manifold+n_var-n_warm_up\n",
    "n_test = 100\n",
    "\n",
    "manifold_data = d[n_warm_up:n_manifold].copy()\n",
    "ic_data = d[n_warm_up:n_ic].copy()\n",
    "max_manifold = np.max(np.abs(manifold_data))\n",
    "print(max_manifold)\n",
    "manifold_data = manifold_data/max_manifold\n",
    "ic_data = ic_data/max_manifold\n",
    "var_data = d[n_manifold:n_manifold+n_var].copy()/max_manifold\n",
    "test_data = d[n_manifold+n_var:n_manifold+n_var+n_test].copy()/max_manifold\n",
    "true_sigma = 1/max_manifold\n",
    "print(true_sigma)\n",
    "\n",
    "ic_mean = np.mean(d/max_manifold,axis=0)\n",
    "ic_std = np.std(d/max_manifold,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.66022095e-01 3.66022070e-01 3.66022693e-01 3.66022742e-01\n",
      " 2.84683986e-01 5.31304459e-02 3.25353146e-01 2.18798292e-02\n",
      " 1.80832586e-02 1.65188357e-02 1.64191264e-02 1.50254195e-02\n",
      " 1.31848068e-02 1.84357219e-02 7.59158011e-01 1.05748212e-02\n",
      " 9.99647016e-01 1.01383240e-02 7.48482321e-01 4.06631982e-03\n",
      " 7.33273849e-01 1.64389385e-02 4.06690199e-03 9.03410048e-01\n",
      " 1.61911087e-02 5.42054164e-01 8.26140570e-07 1.11032176e-01\n",
      " 8.33442434e-01 7.51232909e-01]\n",
      "[8.08511320e-05 8.13454022e-05 8.20397890e-05 8.12428877e-05\n",
      " 8.13356863e-05 1.17577895e-04 8.16856463e-05 1.27233735e-03\n",
      " 3.86759215e-04 8.57832650e-05 8.77932603e-05 8.44339760e-05\n",
      " 8.25819234e-05 8.23006925e-05 2.67795800e-04 8.12398970e-05\n",
      " 8.85728013e-05 4.71613323e-04 5.67356024e-04 8.21500558e-05\n",
      " 2.66793051e-04 8.22607509e-05 8.29022638e-05 3.08626045e-04\n",
      " 8.17515408e-05 3.80167858e-04 8.08347707e-05 1.45636697e-04\n",
      " 1.60743294e-04 1.22170788e-03]\n"
     ]
    }
   ],
   "source": [
    "ic_mean = np.mean(d[:,:30]/max_manifold,axis=0)\n",
    "ic_std = np.std(d[:,:30]/max_manifold,axis=0)\n",
    "print(ic_mean)\n",
    "print(ic_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_dr = os.getcwd()\n",
    "pickle_path = os.path.join(te_dr, r\"TEpkl\")\n",
    "data_path = os.path.join(te_dr, r\"Data\")\n",
    "from pathlib import Path\n",
    "\n",
    "keep_mask_small = keep_mask_temp[:int(len(keep_mask_temp) / 10)]\n",
    "obs_order_small = obs_order[:int(len(obs_order) / 10)]\n",
    "def extract_mean(file):\n",
    "    d = pd.read_csv(file, header=None).to_numpy()[:,1:]\n",
    "    d = d/obs_order_small\n",
    "    d = d/max_manifold\n",
    "    d = d[:,keep_mask_small]\n",
    "    d_oc = d[1200:]\n",
    "    return np.mean(d_oc, axis=0)\n",
    "\n",
    "def shift_size(data_path,fault_no,amplitude):\n",
    "    csv_files = list(Path(data_path).glob(\"*.csv\"))\n",
    "    data = [parse_filename(file) for file in csv_files]\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df[\"fault\"] = df[\"fault\"].astype(int)\n",
    "    df[\"amplitude\"] = df[\"amplitude\"].astype(float)\n",
    "    df[\"RunID\"] = df[\"RunID\"].astype(int)\n",
    "    df = df.sort_values(by=\"RunID\")\n",
    "    faulty_files = df[(df[\"fault\"] == fault_no) & (df[\"amplitude\"] == amplitude)]\n",
    "    faulty_files = faulty_files[:100]\n",
    "    oc_mean = []\n",
    "    for _, row in faulty_files.iterrows():\n",
    "        mean_vals = extract_mean(row[\"path\"])\n",
    "        oc_mean.append(mean_vals)\n",
    "\n",
    "    if oc_mean:\n",
    "        return np.max(np.abs(np.mean(oc_mean, axis=0)-ic_mean)/ic_std)\n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "te_dr = os.getcwd()\n",
    "pickle_path = os.path.join(te_dr, r\"TEpkl\")\n",
    "data_path = os.path.join(te_dr, r\"Data\")\n",
    "from pathlib import Path\n",
    "\n",
    "keep_mask_small = keep_mask_temp[:int(len(keep_mask_temp) / 10)]\n",
    "obs_order_small = obs_order[:int(len(obs_order) / 10)]\n",
    "def extract_mean_ic(file):\n",
    "    d = pd.read_csv(file, header=None).to_numpy()[:,1:]\n",
    "    d = d/obs_order_small\n",
    "    d = d/max_manifold\n",
    "    d = d[:,keep_mask_small]\n",
    "    d_oc = d[:1200]\n",
    "    return np.mean(d_oc, axis=0)\n",
    "\n",
    "def shift_size_ic(data_path,fault_no,amplitude):\n",
    "    csv_files = list(Path(data_path).glob(\"*.csv\"))\n",
    "    data = [parse_filename(file) for file in csv_files]\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df[\"fault\"] = df[\"fault\"].astype(int)\n",
    "    df[\"amplitude\"] = df[\"amplitude\"].astype(float)\n",
    "    df[\"RunID\"] = df[\"RunID\"].astype(int)\n",
    "    df = df.sort_values(by=\"RunID\")\n",
    "    faulty_files = df[(df[\"fault\"] == fault_no) & (df[\"amplitude\"] == amplitude)]\n",
    "    faulty_files = faulty_files[:100]\n",
    "    oc_mean = []\n",
    "    for _, row in faulty_files.iterrows():\n",
    "        mean_vals = extract_mean_ic(row[\"path\"])\n",
    "        oc_mean.append(mean_vals)\n",
    "\n",
    "    if oc_mean:\n",
    "        return np.max(np.abs(np.mean(oc_mean, axis=0)-ic_mean)/ic_std)\n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# ==== Keep these utility functions unchanged ====\n",
    "\n",
    "\n",
    "def extract_mean(file):\n",
    "    d = pd.read_csv(file, header=None).to_numpy()[:, 1:]\n",
    "    d = d[:, keep_mask_temp]  # Assume keep_mask_temp is globally defined\n",
    "    d_oc = d[1200:]\n",
    "    return np.mean(d_oc, axis=0)\n",
    "\n",
    "# ==== Shift Size Calculation Function ====\n",
    "\n",
    "def shift_size(data_path, fault_no, amplitude):\n",
    "    csv_files = list(Path(data_path).glob(\"*.csv\"))\n",
    "    data = [parse_filename(file) for file in csv_files]\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df[\"fault\"] = df[\"fault\"].astype(int)\n",
    "    df[\"amplitude\"] = df[\"amplitude\"].astype(float)\n",
    "    df[\"RunID\"] = df[\"RunID\"].astype(int)\n",
    "    df = df.sort_values(by=\"RunID\")\n",
    "\n",
    "    faulty_files = df[(df[\"fault\"] == fault_no) & (df[\"amplitude\"] == amplitude)]\n",
    "    faulty_files = faulty_files[:100]\n",
    "\n",
    "    oc_mean = []\n",
    "    for _, row in faulty_files.iterrows():\n",
    "        mean_vals = extract_mean(row[\"path\"])\n",
    "        oc_mean.append(mean_vals)\n",
    "\n",
    "    if oc_mean:\n",
    "        return np.max(np.abs(np.mean(oc_mean, axis=0) - ic_mean) / ic_std)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# ==== ARL Processing Function ====\n",
    "\n",
    "def process_experiment(experiment_name, base_path):\n",
    "    experiment_folder = os.path.join(base_path, f\"{experiment_name}\", \"RunLengths\")\n",
    "    methods = [\"LPP\", \"NPE\", \"MF\", \"PCA\", \"AM\"]\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for method in methods:\n",
    "        method_path = os.path.join(experiment_folder, method)\n",
    "\n",
    "        if not os.path.exists(method_path):\n",
    "            print(f\"Skipping missing folder: {method_path}\")\n",
    "            continue\n",
    "\n",
    "        data = []\n",
    "\n",
    "        for file in os.listdir(method_path):\n",
    "            if file.endswith(\".csv\"):\n",
    "                amplitude, rl = extract_values(file)\n",
    "                if amplitude is not None:\n",
    "                    data.append((amplitude, rl))\n",
    "\n",
    "        if data:\n",
    "            df = pd.DataFrame(data, columns=[\"Amplitude\", \"RL\"])\n",
    "            stats = df.groupby(\"Amplitude\")[\"RL\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "            stats.rename(columns={\"mean\": \"RL\", \"std\": \"Std\"}, inplace=True)\n",
    "            stats[\"Method\"] = method\n",
    "            results.append(stats)\n",
    "\n",
    "    if results:\n",
    "        final_df = pd.concat(results, ignore_index=True)\n",
    "        return final_df\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"Amplitude\", \"RL\", \"Std\", \"Method\"])\n",
    "\n",
    "\n",
    "# ==== Main Loop for Faults ====\n",
    "\n",
    "def collect_fault_tables(base_path, data_path):\n",
    "    all_results = []\n",
    "\n",
    "    fault_list = list(range(1, 6)) + list(range(7, 29))  # 1–5 and 7–28\n",
    "    for fault_no in fault_list:\n",
    "        experiment_name = f\"Exptestf{fault_no}_100\"\n",
    "        print(f\"Processing Fault {fault_no}...\")\n",
    "\n",
    "        # Step 1: Get ARL data\n",
    "        arl_df = process_experiment(experiment_name, base_path)\n",
    "\n",
    "        if arl_df.empty:\n",
    "            print(f\"No data for Fault {fault_no}\")\n",
    "            continue\n",
    "\n",
    "        # Step 2: Get shift size\n",
    "        shift = shift_size(data_path, fault_no, amplitude=1.0)\n",
    "\n",
    "        # Step 3: Add FaultNo and ShiftSize to every row\n",
    "        arl_df[\"Fault\"] = fault_no\n",
    "        arl_df[\"ShiftSize\"] = shift\n",
    "\n",
    "        all_results.append(arl_df)\n",
    "\n",
    "    if all_results:\n",
    "        final_table = pd.concat(all_results, ignore_index=True)\n",
    "        return final_table\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"Fault\", \"Amplitude\", \"RL\", \"Std\", \"Method\", \"ShiftSize\"])\n",
    "\n",
    "# ==== Usage ====\n",
    "\n",
    "base_path = r\"C:\\Users\\ibura\\Manifold SPC Paper v3\\TE Simulations\\Experiments\"\n",
    "data_path = r\"C:\\Users\\ibura\\Manifold SPC Paper v3\\TE Simulations\\Data\"\n",
    "\n",
    "final_result = collect_fault_tables(base_path, data_path)\n",
    "print(final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fault 1...\n",
      "Processing Fault 2...\n",
      "Processing Fault 3...\n",
      "Processing Fault 4...\n",
      "Processing Fault 5...\n",
      "Processing Fault 7...\n",
      "Processing Fault 8...\n",
      "Processing Fault 9...\n",
      "Processing Fault 10...\n",
      "Processing Fault 11...\n",
      "Processing Fault 12...\n",
      "Processing Fault 13...\n",
      "Processing Fault 14...\n",
      "Processing Fault 15...\n",
      "Processing Fault 16...\n",
      "Processing Fault 17...\n",
      "Processing Fault 18...\n",
      "Processing Fault 19...\n",
      "Processing Fault 20...\n",
      "Processing Fault 21...\n",
      "Processing Fault 22...\n",
      "Processing Fault 23...\n",
      "Processing Fault 24...\n",
      "Processing Fault 25...\n",
      "Processing Fault 26...\n",
      "Processing Fault 27...\n",
      "Processing Fault 28...\n",
      "     Amplitude     RL        Std Method  Fault  ShiftSize\n",
      "0          1.0   5.75   1.866098    LPP      1  85.052531\n",
      "1          1.0   5.40   1.814643    NPE      1  85.052531\n",
      "2          1.0   5.54   1.816979     MF      1  85.052531\n",
      "3          1.0   5.97   1.898724    PCA      1  85.052531\n",
      "4          1.0   9.11   5.283661    LPP      2  17.576024\n",
      "..         ...    ...        ...    ...    ...        ...\n",
      "103        1.0  16.99  17.059903    PCA     27   0.065559\n",
      "104        1.0  15.95  16.101556    LPP     28   0.047934\n",
      "105        1.0  16.28  19.322923    NPE     28   0.047934\n",
      "106        1.0  18.72  17.450119     MF     28   0.047934\n",
      "107        1.0  17.13  16.718498    PCA     28   0.047934\n",
      "\n",
      "[108 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def extract_values(filename):\n",
    "    match = re.search(r\"amplitude-([\\d\\.-]+)--rl-(\\d+)\", filename)\n",
    "    if match:\n",
    "        amplitude = float(match.group(1))\n",
    "        rl = int(match.group(2))\n",
    "        return amplitude, rl\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def process_experiment(experiment_name, base_path):\n",
    "    experiment_folder = os.path.join(base_path, f\"{experiment_name}\", \"RunLengths\")\n",
    "    methods = [\"LPP\", \"NPE\", \"MF\", \"PCA\"]\n",
    "\n",
    "    results = []\n",
    "    for method in methods:\n",
    "        method_path = os.path.join(experiment_folder, method)\n",
    "        if not os.path.exists(method_path):\n",
    "            print(f\"Skipping missing folder: {method_path}\")\n",
    "            continue\n",
    "\n",
    "        data = []\n",
    "        for file in os.listdir(method_path):\n",
    "            if file.endswith(\".csv\"):\n",
    "                amplitude, rl = extract_values(file)\n",
    "                if amplitude is not None:\n",
    "                    data.append((amplitude, rl))\n",
    "\n",
    "        if data:\n",
    "            df = pd.DataFrame(data, columns=[\"Amplitude\", \"RL\"])\n",
    "            stats = df.groupby(\"Amplitude\")[\"RL\"].agg([\"mean\", \"std\"]).reset_index()\n",
    "            stats.rename(columns={\"mean\": \"RL\", \"std\": \"Std\"}, inplace=True)\n",
    "            stats[\"Method\"] = method\n",
    "            results.append(stats)\n",
    "\n",
    "    if results:\n",
    "        return pd.concat(results, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"Amplitude\", \"RL\", \"Std\", \"Method\"])\n",
    "\n",
    "\n",
    "# === Final loop to generate one ARL+Shift table per fault ===\n",
    "def collect_fault_tables(base_path, data_path):\n",
    "    all_results = []\n",
    "    fault_list = list(range(1, 6)) + list(range(7, 29))  # 1–5 and 7–28\n",
    "\n",
    "    for fault_no in fault_list:\n",
    "        print(f\"Processing Fault {fault_no}...\")\n",
    "        experiment_name = f\"Exptestf{fault_no}_100\"\n",
    "\n",
    "        # Step 1: ARL Table\n",
    "        arl_df = process_experiment(experiment_name, base_path)\n",
    "        if arl_df.empty:\n",
    "            print(f\"No ARL data for fault {fault_no}\")\n",
    "            continue\n",
    "\n",
    "        # Step 2: Shift Size\n",
    "        shift = shift_size(data_path, fault_no, amplitude=1.0)\n",
    "\n",
    "        # Step 3: Merge\n",
    "        arl_df[\"Fault\"] = fault_no\n",
    "        arl_df[\"ShiftSize\"] = shift\n",
    "\n",
    "        all_results.append(arl_df)\n",
    "\n",
    "    if all_results:\n",
    "        return pd.concat(all_results, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame(columns=[\"Fault\", \"Amplitude\", \"RL\", \"Std\", \"Method\", \"ShiftSize\"])\n",
    "\n",
    "\n",
    "# === Usage ===\n",
    "base_path = r\"C:\\Users\\ibura\\Manifold SPC Paper v3\\TE Simulations\\Experiments\"\n",
    "final_result = collect_fault_tables(base_path, data_path)\n",
    "print(final_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_arl_table(df):\n",
    "    # Keep only required methods\n",
    "    methods = [\"LPP\", \"MF\", \"NPE\", \"PCA\"]\n",
    "    df = df[df[\"Method\"].isin(methods)]\n",
    "\n",
    "    # Format ARL with std in parentheses\n",
    "    df[\"ARL_Formatted\"] = df.apply(lambda row: f'{row[\"RL\"]:.1f} ({row[\"Std\"]:.1f})', axis=1)\n",
    "\n",
    "    # Pivot the table: Fault, Amplitude, ShiftSize as index, methods as columns\n",
    "    pivot_df = df.pivot_table(index=[\"Fault\", \"Amplitude\", \"ShiftSize\"],\n",
    "                              columns=\"Method\",\n",
    "                              values=\"ARL_Formatted\",\n",
    "                              aggfunc=\"first\").reset_index()\n",
    "\n",
    "    # Optional: reorder columns\n",
    "    method_order = [\"LPP\", \"MF\", \"NPE\", \"PCA\"]\n",
    "    pivot_df = pivot_df[[\"Fault\", \"Amplitude\", \"ShiftSize\"] + [m for m in method_order if m in pivot_df.columns]]\n",
    "\n",
    "    return pivot_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Fault 1...\n",
      "Processing Fault 2...\n",
      "Processing Fault 3...\n",
      "Processing Fault 4...\n",
      "Processing Fault 5...\n",
      "Processing Fault 7...\n",
      "Processing Fault 8...\n",
      "Processing Fault 9...\n",
      "Processing Fault 10...\n",
      "Processing Fault 11...\n",
      "Processing Fault 12...\n",
      "Processing Fault 13...\n",
      "Processing Fault 14...\n",
      "Processing Fault 15...\n",
      "Processing Fault 16...\n",
      "Processing Fault 17...\n",
      "Processing Fault 18...\n",
      "Processing Fault 19...\n",
      "Processing Fault 20...\n",
      "Processing Fault 21...\n",
      "Processing Fault 22...\n",
      "Processing Fault 23...\n",
      "Processing Fault 24...\n",
      "Processing Fault 25...\n",
      "Processing Fault 26...\n",
      "Processing Fault 27...\n",
      "Processing Fault 28...\n",
      "Method  Fault  Amplitude   ShiftSize          LPP           MF          NPE  \\\n",
      "0           1        1.0   85.052531    5.8 (1.9)    5.5 (1.8)    5.4 (1.8)   \n",
      "1           2        1.0   17.576024    9.1 (5.3)    8.2 (4.1)    9.4 (5.1)   \n",
      "2           3        1.0  497.883546    1.6 (0.5)    2.0 (0.7)    1.7 (0.4)   \n",
      "3           4        1.0  497.935977    1.7 (0.5)    1.9 (0.7)    1.7 (0.5)   \n",
      "4           5        1.0  495.802301    1.7 (0.5)    2.1 (0.7)    1.6 (0.5)   \n",
      "5           7        1.0    0.245283    3.2 (1.1)    2.3 (0.7)    3.0 (1.0)   \n",
      "6           8        1.0    1.627202   12.2 (8.4)   13.7 (7.6)   12.6 (8.8)   \n",
      "7           9        1.0    7.385667    8.3 (4.4)    7.7 (4.4)    7.3 (4.3)   \n",
      "8          10        1.0    3.350745   11.5 (8.1)   12.1 (8.8)   11.3 (8.4)   \n",
      "9          11        1.0   15.427773    4.9 (2.4)    5.0 (2.5)    4.9 (2.2)   \n",
      "10         12        1.0    9.519076    5.2 (2.6)    5.5 (2.5)    5.8 (2.6)   \n",
      "11         13        1.0   21.706332  16.6 (12.4)  15.5 (12.1)  16.0 (12.5)   \n",
      "12         14        1.0    0.129658  24.2 (31.3)    5.3 (3.2)  26.9 (41.3)   \n",
      "13         15        1.0    0.063068  18.6 (17.7)  15.6 (14.7)  19.3 (16.8)   \n",
      "14         16        1.0    0.090231  23.8 (23.6)  21.4 (25.6)  18.3 (15.0)   \n",
      "15         17        1.0  135.194618  17.0 (14.0)  16.0 (12.6)  15.9 (12.6)   \n",
      "16         18        1.0   16.016436  17.7 (14.2)  15.3 (15.4)  19.1 (13.4)   \n",
      "17         19        1.0    0.083514  17.7 (14.0)  20.1 (21.3)  19.6 (19.4)   \n",
      "18         20        1.0   15.594339   13.9 (9.7)  14.5 (11.1)  15.2 (11.0)   \n",
      "19         21        1.0    5.435333    5.0 (2.4)    5.6 (2.5)    5.1 (2.5)   \n",
      "20         22        1.0   16.183633    7.6 (4.6)    7.6 (4.4)    7.2 (4.5)   \n",
      "21         23        1.0    0.042952  21.6 (21.6)  22.6 (31.4)  20.9 (19.5)   \n",
      "22         24        1.0    0.078762   13.1 (8.4)   11.8 (6.9)   11.2 (7.9)   \n",
      "23         25        1.0    0.096586  14.9 (11.6)  18.4 (17.3)  16.7 (12.6)   \n",
      "24         26        1.0    0.099693  16.0 (12.8)  21.3 (19.8)  20.5 (20.6)   \n",
      "25         27        1.0    0.065559  12.9 (12.7)  14.9 (12.9)  16.1 (14.5)   \n",
      "26         28        1.0    0.047934  15.9 (16.1)  18.7 (17.5)  16.3 (19.3)   \n",
      "\n",
      "Method          PCA  \n",
      "0         6.0 (1.9)  \n",
      "1        10.8 (6.4)  \n",
      "2       15.0 (14.8)  \n",
      "3       14.2 (17.4)  \n",
      "4         4.6 (2.0)  \n",
      "5         4.4 (1.2)  \n",
      "6        13.0 (8.1)  \n",
      "7       16.4 (14.2)  \n",
      "8       18.7 (15.8)  \n",
      "9       14.2 (12.2)  \n",
      "10       10.2 (6.8)  \n",
      "11      16.1 (12.6)  \n",
      "12      20.3 (20.0)  \n",
      "13      18.2 (19.4)  \n",
      "14      18.8 (17.3)  \n",
      "15      19.5 (15.1)  \n",
      "16      16.8 (13.0)  \n",
      "17      18.1 (17.0)  \n",
      "18      15.8 (12.5)  \n",
      "19      14.9 (13.0)  \n",
      "20      17.8 (16.7)  \n",
      "21      19.1 (18.8)  \n",
      "22       12.5 (8.2)  \n",
      "23      17.8 (15.7)  \n",
      "24      17.9 (15.7)  \n",
      "25      17.0 (17.1)  \n",
      "26      17.1 (16.7)  \n"
     ]
    }
   ],
   "source": [
    "# Step 1: Collect full raw result table\n",
    "final_result = collect_fault_tables(base_path, data_path)\n",
    "\n",
    "# Step 2: Format it as described\n",
    "summary_table = format_arl_table(final_result)\n",
    "\n",
    "# Step 3: View or save\n",
    "print(summary_table)\n",
    "# summary_table.to_csv(\"ARL_summary_table.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method  Fault  Amplitude   ShiftSize          LPP           MF          NPE  \\\n",
      "0           1        1.0   85.052531    5.8 (1.9)    5.5 (1.8)    5.4 (1.8)   \n",
      "1           2        1.0   17.576024    9.1 (5.3)    8.2 (4.1)    9.4 (5.1)   \n",
      "2           3        1.0  497.883546    1.6 (0.5)    2.0 (0.7)    1.7 (0.4)   \n",
      "3           4        1.0  497.935977    1.7 (0.5)    1.9 (0.7)    1.7 (0.5)   \n",
      "4           5        1.0  495.802301    1.7 (0.5)    2.1 (0.7)    1.6 (0.5)   \n",
      "5           7        1.0    0.245283    3.2 (1.1)    2.3 (0.7)    3.0 (1.0)   \n",
      "6           8        1.0    1.627202   12.2 (8.4)   13.7 (7.6)   12.6 (8.8)   \n",
      "7           9        1.0    7.385667    8.3 (4.4)    7.7 (4.4)    7.3 (4.3)   \n",
      "8          10        1.0    3.350745   11.5 (8.1)   12.1 (8.8)   11.3 (8.4)   \n",
      "9          11        1.0   15.427773    4.9 (2.4)    5.0 (2.5)    4.9 (2.2)   \n",
      "10         12        1.0    9.519076    5.2 (2.6)    5.5 (2.5)    5.8 (2.6)   \n",
      "11         13        1.0   21.706332  16.6 (12.4)  15.5 (12.1)  16.0 (12.5)   \n",
      "12         14        1.0    0.129658  24.2 (31.3)    5.3 (3.2)  26.9 (41.3)   \n",
      "13         15        1.0    0.063068  18.6 (17.7)  15.6 (14.7)  19.3 (16.8)   \n",
      "14         16        1.0    0.090231  23.8 (23.6)  21.4 (25.6)  18.3 (15.0)   \n",
      "15         17        1.0  135.194618  17.0 (14.0)  16.0 (12.6)  15.9 (12.6)   \n",
      "16         18        1.0   16.016436  17.7 (14.2)  15.3 (15.4)  19.1 (13.4)   \n",
      "17         19        1.0    0.083514  17.7 (14.0)  20.1 (21.3)  19.6 (19.4)   \n",
      "18         20        1.0   15.594339   13.9 (9.7)  14.5 (11.1)  15.2 (11.0)   \n",
      "19         21        1.0    5.435333    5.0 (2.4)    5.6 (2.5)    5.1 (2.5)   \n",
      "20         22        1.0   16.183633    7.6 (4.6)    7.6 (4.4)    7.2 (4.5)   \n",
      "21         23        1.0    0.042952  21.6 (21.6)  22.6 (31.4)  20.9 (19.5)   \n",
      "22         24        1.0    0.078762   13.1 (8.4)   11.8 (6.9)   11.2 (7.9)   \n",
      "23         25        1.0    0.096586  14.9 (11.6)  18.4 (17.3)  16.7 (12.6)   \n",
      "24         26        1.0    0.099693  16.0 (12.8)  21.3 (19.8)  20.5 (20.6)   \n",
      "25         27        1.0    0.065559  12.9 (12.7)  14.9 (12.9)  16.1 (14.5)   \n",
      "26         28        1.0    0.047934  15.9 (16.1)  18.7 (17.5)  16.3 (19.3)   \n",
      "\n",
      "Method          PCA  \n",
      "0         6.0 (1.9)  \n",
      "1        10.8 (6.4)  \n",
      "2       15.0 (14.8)  \n",
      "3       14.2 (17.4)  \n",
      "4         4.6 (2.0)  \n",
      "5         4.4 (1.2)  \n",
      "6        13.0 (8.1)  \n",
      "7       16.4 (14.2)  \n",
      "8       18.7 (15.8)  \n",
      "9       14.2 (12.2)  \n",
      "10       10.2 (6.8)  \n",
      "11      16.1 (12.6)  \n",
      "12      20.3 (20.0)  \n",
      "13      18.2 (19.4)  \n",
      "14      18.8 (17.3)  \n",
      "15      19.5 (15.1)  \n",
      "16      16.8 (13.0)  \n",
      "17      18.1 (17.0)  \n",
      "18      15.8 (12.5)  \n",
      "19      14.9 (13.0)  \n",
      "20      17.8 (16.7)  \n",
      "21      19.1 (18.8)  \n",
      "22       12.5 (8.2)  \n",
      "23      17.8 (15.7)  \n",
      "24      17.9 (15.7)  \n",
      "25      17.0 (17.1)  \n",
      "26      17.1 (16.7)  \n"
     ]
    }
   ],
   "source": [
    "# Step 3: View or save\n",
    "print(summary_table)\n",
    "# summary_table.to_csv(\"ARL_summary_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: View or save\n",
    "print(summary_table)\n",
    "# summary_table.to_csv(\"ARL_summary_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_table.to_csv(\"ARL_summary_table.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "n_dim = 22\n",
    "n_neighbors = 15\n",
    "pca = PCA(n_components=n_dim)\n",
    "pca_ic = pca.fit_transform(manifold_data)\n",
    "\n",
    "lpp = LocalityPreservingProjection(n_components=n_dim,n_neighbors=n_neighbors,weight=\"heat\")\n",
    "lpp_ic = lpp.fit_transform(manifold_data)\n",
    "\n",
    "npe = NeighborhoodPreservingEmbedding(n_components=n_dim,n_neighbors=n_neighbors)\n",
    "npe_ic = npe.fit_transform(manifold_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_lpp = lpp.projection_.T\n",
    "P_npe = npe.components_.T\n",
    "P_pca = pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ibura\\Manifold SPC Paper v3\\TE Simulations\n",
      "c:\\Users\\ibura\\Manifold SPC Paper v3\\TE Simulations\\faults\n"
     ]
    }
   ],
   "source": [
    "te_dr = os.getcwd()\n",
    "faults_folder = \"faults\"\n",
    "print(te_dr)\n",
    "faults_path = os.path.join(te_dr,faults_folder)\n",
    "csv_files = [f for f in os.listdir(faults_path) if f.endswith('.csv')]\n",
    "print(faults_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_the_mean(faults_path,fault_no,amplitude):\n",
    "    \n",
    "    csv_files = list(Path(faults_path).glob(\"*.csv\"))\n",
    "    data = [parse_filename(file) for file in csv_files]\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    df[\"fault\"] = df[\"fault\"].astype(int)\n",
    "    df[\"amplitude\"] = df[\"amplitude\"].astype(float)\n",
    "    df[\"RunID\"] = df[\"RunID\"].astype(int)\n",
    "    df = df.sort_values(by=\"RunID\")\n",
    "\n",
    "    faulty_files = df[(df[\"fault\"] == fault_no) & (df[\"amplitude\"] == amplitude)]\n",
    "    #print(faulty_files)\n",
    "\n",
    "    return extract_mean_oc_with_padding(faulty_files.iloc[0][\"path\"])\n",
    "\n",
    "\n",
    "keep_mask_small = keep_mask_temp[:int(len(keep_mask_temp) / 10)]\n",
    "obs_order_small = obs_order[:int(len(obs_order) / 10)]\n",
    "def extract_mean_oc_with_padding(file):\n",
    "    d = pd.read_csv(file, header=None).to_numpy()[:,1:]\n",
    "    d = d/obs_order_small\n",
    "    d = d/max_manifold\n",
    "    d = d[:,keep_mask_small]\n",
    "    d_oc = d[2000:]\n",
    "    vec = np.mean(d_oc, axis=0)\n",
    "    vec = vec - ic_mean\n",
    "    vec = np.pad(vec, (0, max(0, 300 - len(vec))), mode='constant')\n",
    "    norm = np.linalg.norm(vec)\n",
    "    if norm > 0:\n",
    "        vec = vec\n",
    "    return vec\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(8.895479597392774e-05)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_the_mean(faults_path,14,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import null_space\n",
    "\n",
    "def decompose_vector_by_projection(P, v):\n",
    "    \"\"\"\n",
    "    P: projection matrix (not the full orthogonal projection matrix, but the components_.T like in PCA)\n",
    "    v: vector of shape (n,)\n",
    "    \n",
    "    Returns:\n",
    "        norm_in_projection_space\n",
    "        norm_in_null_space\n",
    "    \"\"\"\n",
    "    # Ensure vector is a numpy array and has correct shape\n",
    "    v = np.asarray(v).flatten()\n",
    "    \n",
    "    # Step 1: Get orthonormal basis for column space of P\n",
    "    Q, _ = np.linalg.qr(P)\n",
    "    \n",
    "    # Step 2: Get orthonormal basis for null space of P\n",
    "    N = null_space(P.T)  # null space of P.T is orthogonal complement of range(P)\n",
    "    \n",
    "    # Step 3: Project vector onto each space\n",
    "    proj_col = Q @ (Q.T @ v)\n",
    "    proj_null = N @ (N.T @ v)\n",
    "    \n",
    "    # Step 4: Compute norms\n",
    "    norm_col = np.linalg.norm(proj_col)\n",
    "    norm_null = np.linalg.norm(proj_null)\n",
    "    \n",
    "    return norm_col, norm_null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.03304931  0.01447655 -0.00613893 -0.00968497  0.00605705  0.19944701\n",
      "  0.00272531  0.42635059 -0.00940727  0.00669853 -0.00449261  0.0206242\n",
      " -0.01857882 -0.0090157   0.0257016  -0.03624111 -0.01107958  0.03310386\n",
      " -0.15795696 -0.02513252  0.01644652  0.02179586  0.02248981  0.01258481\n",
      " -0.03643698 -0.35672688 -0.03020232 -0.09794802  0.33140235 -0.70488557]\n",
      "[ 2.13874762e-06  9.99983718e-01  7.67720315e-06 -3.56130738e-05\n",
      " -3.48837821e-05  2.20523162e-03 -5.81781130e-05  1.09437434e-04\n",
      " -2.74241154e-05  7.28677329e-05 -4.44704832e-05 -2.15818492e-05\n",
      "  2.89089348e-05 -3.02419731e-05 -5.96763774e-05 -6.33402854e-06\n",
      "  5.94598975e-05 -1.37674066e-04  6.31646484e-05  3.33011487e-05\n",
      "  1.72171860e-05 -5.00327207e-05 -3.56338138e-06 -3.03358677e-06\n",
      " -6.75514750e-06 -1.69959747e-04  6.18921019e-05 -2.25951785e-05\n",
      " -5.25368910e-03  6.05818234e-05]\n"
     ]
    }
   ],
   "source": [
    "f_14_normalized = output_the_mean(faults_path,14,1)\n",
    "f_3_normalized = output_the_mean(faults_path,3,1)\n",
    "print(f_14_normalized[:30])\n",
    "print(f_3_normalized[:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(np.float64(0.23510940117249055), np.float64(0.9719689138446311))\n"
     ]
    }
   ],
   "source": [
    "print(decompose_vector_by_projection(P_npe.T,output_the_mean(faults_path,14,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
